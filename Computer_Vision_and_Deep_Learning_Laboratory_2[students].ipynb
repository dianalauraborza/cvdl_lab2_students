{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computer Vision and Deep Learning - Laboratory 2[students].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGpOGQHgVoKM"
      },
      "source": [
        "# Laboratory 2\n",
        "\n",
        "In this second laboratory you will implement (from scratch) and train two simple classifiers: the softmax classifier.\n",
        "\n",
        "Don't worry, starting from the next laboratory we'll be using a popular machine learning library, but for now it is very important to understand the anatomy of these simple models.\n",
        " \n",
        "Another important aspect that we'll be discussing today is how to properly evaluate a classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0--EpYvTEM3"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6YAQldx8ZLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1792b5-c33e-41a3-cffb-38889a7d49b6"
      },
      "source": [
        "!rm -rf cvdl_lab2_students\n",
        "![ -d cvdl_lab2_students ] || git clone https://github.com/dianalauraborza/cvdl_lab2_students.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cvdl_lab2_students'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/28)\u001b[K\rremote: Counting objects:   7% (2/28)\u001b[K\rremote: Counting objects:  10% (3/28)\u001b[K\rremote: Counting objects:  14% (4/28)\u001b[K\rremote: Counting objects:  17% (5/28)\u001b[K\rremote: Counting objects:  21% (6/28)\u001b[K\rremote: Counting objects:  25% (7/28)\u001b[K\rremote: Counting objects:  28% (8/28)\u001b[K\rremote: Counting objects:  32% (9/28)\u001b[K\rremote: Counting objects:  35% (10/28)\u001b[K\rremote: Counting objects:  39% (11/28)\u001b[K\rremote: Counting objects:  42% (12/28)\u001b[K\rremote: Counting objects:  46% (13/28)\u001b[K\rremote: Counting objects:  50% (14/28)\u001b[K\rremote: Counting objects:  53% (15/28)\u001b[K\rremote: Counting objects:  57% (16/28)\u001b[K\rremote: Counting objects:  60% (17/28)\u001b[K\rremote: Counting objects:  64% (18/28)\u001b[K\rremote: Counting objects:  67% (19/28)\u001b[K\rremote: Counting objects:  71% (20/28)\u001b[K\rremote: Counting objects:  75% (21/28)\u001b[K\rremote: Counting objects:  78% (22/28)\u001b[K\rremote: Counting objects:  82% (23/28)\u001b[K\rremote: Counting objects:  85% (24/28)\u001b[K\rremote: Counting objects:  89% (25/28)\u001b[K\rremote: Counting objects:  92% (26/28)\u001b[K\rremote: Counting objects:  96% (27/28)\u001b[K\rremote: Counting objects: 100% (28/28)\u001b[K\rremote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects:   5% (1/20)\u001b[K\rremote: Compressing objects:  10% (2/20)\u001b[K\rremote: Compressing objects:  15% (3/20)\u001b[K\rremote: Compressing objects:  20% (4/20)\u001b[K\rremote: Compressing objects:  25% (5/20)\u001b[K\rremote: Compressing objects:  30% (6/20)\u001b[K\rremote: Compressing objects:  35% (7/20)\u001b[K\rremote: Compressing objects:  40% (8/20)\u001b[K\rremote: Compressing objects:  45% (9/20)\u001b[K\rremote: Compressing objects:  50% (10/20)\u001b[K\rremote: Compressing objects:  55% (11/20)\u001b[K\rremote: Compressing objects:  60% (12/20)\u001b[K\rremote: Compressing objects:  65% (13/20)\u001b[K\rremote: Compressing objects:  70% (14/20)\u001b[K\rremote: Compressing objects:  75% (15/20)\u001b[K\rremote: Compressing objects:  80% (16/20)\u001b[K\rremote: Compressing objects:  85% (17/20)\u001b[K\rremote: Compressing objects:  90% (18/20)\u001b[K\rremote: Compressing objects:  95% (19/20)\u001b[K\rremote: Compressing objects: 100% (20/20)\u001b[K\rremote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "Unpacking objects:   3% (1/28)   \rUnpacking objects:   7% (2/28)   \rUnpacking objects:  10% (3/28)   \rUnpacking objects:  14% (4/28)   \rUnpacking objects:  17% (5/28)   \rUnpacking objects:  21% (6/28)   \rUnpacking objects:  25% (7/28)   \rUnpacking objects:  28% (8/28)   \rremote: Total 28 (delta 8), reused 26 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects:  32% (9/28)   \rUnpacking objects:  35% (10/28)   \rUnpacking objects:  39% (11/28)   \rUnpacking objects:  42% (12/28)   \rUnpacking objects:  46% (13/28)   \rUnpacking objects:  50% (14/28)   \rUnpacking objects:  53% (15/28)   \rUnpacking objects:  57% (16/28)   \rUnpacking objects:  60% (17/28)   \rUnpacking objects:  64% (18/28)   \rUnpacking objects:  67% (19/28)   \rUnpacking objects:  71% (20/28)   \rUnpacking objects:  75% (21/28)   \rUnpacking objects:  78% (22/28)   \rUnpacking objects:  82% (23/28)   \rUnpacking objects:  85% (24/28)   \rUnpacking objects:  89% (25/28)   \rUnpacking objects:  92% (26/28)   \rUnpacking objects:  96% (27/28)   \rUnpacking objects: 100% (28/28)   \rUnpacking objects: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpUQJM2vFzC1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d6bb5101-b151-4942-d472-328e71130d73"
      },
      "source": [
        "from google.colab import files\n",
        "files.view('cvdl_lab2_students/lab2/metrics.py')\n",
        "files.view('cvdl_lab2_students/lab2/softmax.py')\n",
        "files.view('cvdl_lab2_students/lab2/activations.py')\n",
        "\n",
        "import sys\n",
        "if './cvdl_lab2_students' not in sys.path:\n",
        "  sys.path.append('./cvdl_lab2_students')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/cvdl_lab2_students/lab2/metrics.py\")"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/cvdl_lab2_students/lab2/softmax.py\")"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/cvdl_lab2_students/lab2/activations.py\")"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxfMZRro1rM9"
      },
      "source": [
        "## Warm-up\n",
        "\n",
        "Let's start by implementing the softmax function. This function takes as input an array of *N* arbitrary numbers and normalizes the array such that the output is a probability distribution.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "softmax(x)_i = \\frac{e^{x_i}}{\\sum_{j = 0}^{j = N} e^{x_j}}\n",
        "\\end{equation}\n",
        "\n",
        "In the file *activations.py* write the implementation of the softmax function.\n",
        "\n",
        "**Short discussion about softmax numerical stability.** You can also check this [post](https://ogunlao.github.io/2020/04/26/you_dont_really_know_softmax.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opIOjvif1g0G"
      },
      "source": [
        "# validate softmax\n",
        "from lab2.activations import softmax\n",
        "\n",
        "# let's check that you obtained the same values \n",
        "# as the softmax implementation in tensorflow\n",
        "arr = np.asarray([2, 4, 10, 100, 2.0])\n",
        "assert (np.allclose(tf.nn.softmax(arr).numpy(), softmax(arr)))\n",
        "arr = np.asarray([0.0, 0, 0, 1, 0])\n",
        "assert (np.allclose(tf.nn.softmax(arr).numpy(), softmax(arr)))\n",
        "arr = np.asarray([-750.0, 23, 9, 10, 230])\n",
        "assert (np.allclose(tf.nn.softmax(arr).numpy(), softmax(arr)))\n",
        "arr = np.ones((4, ))\n",
        "assert (np.allclose(tf.nn.softmax(arr).numpy(), softmax(arr)))\n",
        "arr = np.zeros((4, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK_G-LNe1kW2"
      },
      "source": [
        "*Softmax temperature* is a concept that we'll be using later in this course. \n",
        "The *softmax temperature* is a hyper-parameter (positive number) which scales the input of the softmax function to modify the output probabilities.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "softmax(x, T)_i = \\frac{e^{x_i/T}}{\\sum_{j = 0}^{j = N} e^{x_j/T}}\n",
        "\\end{equation}\n",
        "\n",
        "Now modify your implementation of the softmax function such that it also takes as input the softmax temperature (a positive floating point number). If this parameter is not specified, it should default to 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYt1HAeE3Joc"
      },
      "source": [
        "Now let's visualise what is the effect of the softmax temperature. \n",
        "Given the input vector *x = [100, 2, -150, 75, 99, 3]* , plot the original vector and the softmax with temperatures $ T \\in \\{0.25, 0.75, 1, 1.5, 2, 5, 10, 20, 30\\} $.\n",
        " \n",
        "You can use a bar plot for this. The title for each plot should be the value of the softmax temperature. Also, make sure that for all the plots the range of the *y* axis is set to (0, 1).\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCgonLQB60lH"
      },
      "source": [
        "# your code here\n",
        "import matplotlib.pyplot as plt\n",
        "x = np.asarray([20, 30, -15, 45, 39, -10])\n",
        "T = [0.25, 0.75, 1, 1.5, 2, 5, 10, 20, 30]\n",
        "\n",
        "for idx in range(0, len(T)):\n",
        "  # TODO your code here\n",
        "  # plot the result of applying the softmax function \n",
        "  # with different temperatures on the array x\n",
        "  \n",
        "  # end TODO your code here\n",
        "  pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssG32V9U3dZt"
      },
      "source": [
        "Analyse the plots and answer the following questions:\n",
        "* What happens when we use a large number for the softmax temperature?\n",
        "* What happens when we use a small number (i.e. less than 1) for the softmax temperature?\n",
        "* In the context of image classification, the predicted class is determined by taking the *argmax* of the softmax function. Does the softmax temperature change in any way this prediction?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOMOuotipyB6"
      },
      "source": [
        "## Image classification\n",
        " \n",
        "Image classification refers to the problem of automatically assigning a label (a class) to an image $I_i$. As classification is a supervised learning problem, each image is assigned a ground truth label $y_i \\in \\{1, 2, ..., C\\}$.\n",
        " \n",
        " \n",
        "A linear classifier uses a function $f$ to map an input image $I_i \\in R^{H\\times W\\times 3}$ into a vector of C class scores  $\\hat y_i \\in R^C$:\n",
        " \n",
        "\\begin{equation}\n",
        "f(I_i) = X_i \\cdot W + b\n",
        "\\end{equation}\n",
        " \n",
        "The learnable parameters of the classifier are the weight matrix $W$ and the bias vector $b$.\n",
        " \n",
        "During the training process, the values of the weight matrix and the bias vector are learned by minimizing a loss function (that penalises the discrepancy between the predicted and the ground truth label).\n",
        " \n",
        "In your implementation, use the bias trick to include the bias term in the weight matrix.\n",
        " \n",
        "## Implementation\n",
        " \n",
        " \n",
        "### Softmax classification\n",
        " \n",
        "We'll follow an object oriented approach to solve this problem.\n",
        "All the code related to the softmax classifier will be implemented in the class *SoftmaxClassifier* (defined in the script _softmax.py_).\n",
        " \n",
        "The class comprises the following methods:\n",
        " \n",
        "| Method                               | Description |\n",
        "| ----------- | ----------- |\n",
        "| initialize()      | This function randomly initializes the weights of the linear classifier.    |\n",
        "| fit(X_train, y_train, **kwargs)      | This function will learn the weights of the model based on the training data samples (X_train) and their corresponding ground truth (y_train)       |\n",
        "|  predict(X)                | This function will return the classifier's prediction (the predicted class) for the data passed as parameter.        |\n",
        "|  predict_proba(X)                | This function will return the classifier's predictions for the data passed as parameters.        |\n",
        "|  save(path)   | This function will dump the weights of the classifier in the path specified as parameter.        |\n",
        "|  load(load)   | This function will load the classifier's weights from the path specified as parameter      |\n",
        " \n",
        "Feel free to add any additional helper methods if needed.\n",
        " \n",
        "#### Initialization and persistence \n",
        "* the constructor of this class takes as input the flattened size of the input image and the number of output classes, sets the corresponding class variables and calls the _init()_ method.\n",
        "* fill in the _initialize()_ method. You figure out the shape of the weight matrix based on the input shape and the number of classes. Initialize the weight matrix with small random variables.\n",
        "__Use the bias trick__!\n",
        "* fill in the _save_ and _load_ functions. In these functions you should just dump and restore, respectively, the weight matrix to/from the specified file.\n",
        " \n",
        "#### Inference - the _predict_ and _predict\\_proba_ methods\n",
        " \n",
        "The inference is quite simple. You just need to compute the dot product between the input and the weight matrix.\n",
        "You will implement two inference methods:\n",
        "* _predict_ - this will just return the predicted class label. So you just need to compute the dot product and take the argmax of the result.\n",
        "* _predict\\_proba_ - this will return the class probabilities. So after computing the dot product, you also need to apply the softmax function on the result to normalize it to a probability distribution.\n",
        " \n",
        "#### Training  - the _fit_ method\n",
        " \n",
        "The training process is implemented in the function _fit_.\n",
        "The softmax loss function is defined as:\n",
        " \n",
        "\\begin{equation}\n",
        "L_i = - log(s(X_i)_{y_i}),\n",
        "\\end{equation}\n",
        " \n",
        " where s(.) is the softmax function.\n",
        "\n",
        "So, we are basically just taking the negative of the logarithm of the predicted probability for the ground truth class.\n",
        "In the case of the softmax classifier, $\\hat y_i$ is always a value in the interval [0, 1] (we use the softmax function to normalize the logits into a probability distribution). \n",
        " \n",
        "When the probability of the ground truth class is small (i.e. close to 0), the loss will be very high (theoretically, infinite) [log(0) = -inf]. On the other hand, when the probability of the correct class will be high (i.e. close to 1), then the loss will be close to 0 [log(1) = 0].\n",
        " \n",
        "For training the classifier, the loss function must be minimized and we'll achieve this using gradient descent.\n",
        " \n",
        " \n",
        "Using the gradient descent algorithm, the weights of the classifier update by taking small steps in the opposite direction of the gradient of the loss function in the current point:\n",
        " \n",
        "\\begin{equation}\n",
        "W += -\\lambda * dW \n",
        "\\end{equation}\n",
        ", where W is the weight matrix and dW is the gradient with respect to the weight matrix. $\\lambda$ is the learning rate (a hyper-parameter of the model).\n",
        " \n",
        "The first step is to determine the analytical gradient of the loss function with respect to parameters of the classifier. It turns out that the gradient has an elegant and simple form:\n",
        " \n",
        "\\begin{equation}\n",
        "\\frac{\\partial L_i}{\\partial W} = x_j \\cdot (s_i - \\delta_{ij}).\n",
        "\\end{equation}\n",
        " \n",
        "In the equation above $\\delta_{ij}$ is the Kronecker delta function (i.e. the function is 1 if _i_ and _j_ are equal, and 0 otherwise).\n",
        " \n",
        "For a detailed derivation of the gradient for the softmax loss you can check out this [post](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/).\n",
        " \n",
        "Remember from the lecture that, in order to avoid overfitting, a regularization term is added to the loss function. \n",
        " \n",
        "You will implement the $L_2$ regularization, in which you also add the  sum of squares of all of the feature weights to the loss.\n",
        " \n",
        "\\begin{equation}\n",
        "L_t = \\frac{1}{N} \\sum_i -log(s(X_i)_{y_i}) + \\rho \\cdot \\sum_r \\sum_c W_{(r, c)}^2.\n",
        "\\end{equation}\n",
        " \n",
        "$\\rho$ is the regularization strength, another hyper-parameter of the model.\n",
        " \n",
        "The gradient with respect to the regularization term is straight forward.\n",
        "You will use mini-batch gradient descent, in which you will perform the parameter update after \"seeing\" a batch of samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmzh0QjSMind"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We'll be using CIFAR-10 dataset. \n",
        "The dataset comprises 60000 colour images with a resolution of $32 \\times 32$, separated into 10 classes, with 6000 images per class. It is already split into train-test subsets, with 50000 training images and 10000 test images.\n",
        "\n",
        "You can download the data from this [link](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjTfrvpG3b8L"
      },
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xvf cifar-10-python.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIyze33iqbWC"
      },
      "source": [
        "In the script *cifar10.py* you will complete the function *load_cifar10*, which load the data from the archive you just downloaded.\n",
        "\n",
        "The images of this dataset are are stored in a numpy array, one image per row, in the following order:\n",
        "\n",
        "\"_The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image._\"\n",
        "\n",
        "Your task here is just to manipulate this array, such that each image has the shape (32, 32, 3) and uses RGB ordering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrwvGq7bhLXb"
      },
      "source": [
        "Now let's visualize some of the images from the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17wIc7-6hFst"
      },
      "source": [
        "  from lab2 import cifar10\n",
        "  cifar_root_dir = 'cifar-10-batches-py'\n",
        "  _, _, X_test, y_test = cifar10.load_ciaf10(cifar_root_dir)\n",
        "  indices = np.random.choice(len(X_test), 15)\n",
        "\n",
        "  display_images, display_labels = X_test[indices], y_test[indices]\n",
        "  for idx, (img, label) in enumerate(zip(display_images, display_labels)):\n",
        "      plt.subplot(3, 5, idx + 1)\n",
        "      plt.imshow(img)\n",
        "      plt.title(LABELS[label])\n",
        "      plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5U_lzCrht3-"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "By now you have implemented all the blocks needed to train a softmax classifier.\n",
        "\n",
        "Let's train some classifiers using different hyper-parameters.\n",
        "\n",
        "We'll be using a random search to tune the hyper-parameters of the classifier (the learning rate $\\lambda$ and the regularization strength $\\rho$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8RputSQhsZJ"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from lab2 import cifar10 as cifar10\n",
        "from lab2.softmax import SoftmaxClassifier\n",
        "\n",
        "\n",
        "cifar_root_dir = 'cifar-10-batches-py'\n",
        "\n",
        "# the number of trains performed with different hyper-parameters\n",
        "search_iter = 10\n",
        "# the batch size\n",
        "batch_size = 200\n",
        "# number of training steps per training process\n",
        "train_steps = 5000\n",
        "\n",
        "# load cifar10 dataset\n",
        "X_train, y_train, X_test, y_test = cifar10.load_ciaf10(cifar_root_dir)\n",
        "\n",
        "# convert the training and test data to floating point\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "# Reshape the training data such that we have one image per row\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "# pre-processing: subtract mean image\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "X_train -= mean_image\n",
        "X_test -= mean_image\n",
        "\n",
        "# Bias trick - add 1 to each training example\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "\n",
        "# the search limits for the learning rate and regularization strength\n",
        "# we'll use log scale for the search\n",
        "lr_bounds = (-7, -2)\n",
        "reg_strength_bounds = (-4, -2)\n",
        "\n",
        "if not os.path.exists('train'):\n",
        "    os.mkdir('train')\n",
        "\n",
        "best_acc = -1\n",
        "best_cls_path = ''\n",
        "\n",
        "learning_rates = [-7, -5]\n",
        "regularization_strengths = [3000, 80000]\n",
        "\n",
        "input_size_flattened = reduce((lambda a, b: a * b), X_train[0].shape)\n",
        "results = []\n",
        "\n",
        "for index in range(0, search_iter):\n",
        "    # use log scale for sampling the learning rate\n",
        "    lr = pow(10, random.uniform(learning_rates[0], learning_rates[1]))\n",
        "    reg_strength = random.uniform(regularization_strengths[0], regularization_strengths[1])\n",
        "\n",
        "    cls = SoftmaxClassifier(input_shape=input_size_flattened, num_classes=cifar10.NUM_CLASSES)\n",
        "    history = cls.fit(X_train, y_train, lr=lr, reg_strength=reg_strength,\n",
        "            steps=train_steps, bs=batch_size)\n",
        "\n",
        "    results.append({\n",
        "        'lr': lr,\n",
        "        'reg': reg_strength,\n",
        "        'history': history\n",
        "    })\n",
        "\n",
        "    y_train_pred = cls.predict(X_train)\n",
        "    y_val_pred = cls.predict(X_test)\n",
        "\n",
        "    train_acc = np.mean(y_train == y_train_pred)\n",
        "\n",
        "    test_acc = np.mean(y_test == y_val_pred)\n",
        "    sys.stdout.write('\\rlr {:.4f}, reg_strength{:.2f}, test_acc {:.2f}; train_acc {:.2f}'.format(lr, reg_strength, test_acc, train_acc))\n",
        "    cls_path = os.path.join('train', 'softmax_lr{:.4f}_reg{:.4f}-test{:.2f}.npy'.format(lr, reg_strength, test_acc))\n",
        "    cls.save(cls_path)\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        best_cls_path = cls_path\n",
        "\n",
        "\n",
        "num_rows = search_iter//5 + 1\n",
        "for idx, res in enumerate(results):\n",
        "    plt.subplot(num_rows, 5, idx + 1)\n",
        "    plt.plot(res['history'])\n",
        "plt.show()\n",
        "\n",
        "best_softmax = SoftmaxClassifier(input_shape=input_size_flattened, num_classes=cifar10.NUM_CLASSES)\n",
        "best_softmax.load(best_cls_path)\n",
        "\n",
        "\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "# now let's display the weights for the best model\n",
        "weights = best_softmax.get_weights((32, 32, 3))\n",
        "w_min = np.amin(weights)\n",
        "w_max = np.amax(weights)\n",
        "\n",
        "for idx in range(0, cifar10.NUM_CLASSES):\n",
        "    plt.subplot(2, 5, idx + 1)\n",
        "    # normalize the weights\n",
        "    template = 255.0 * (weights[idx, :, :, :].squeeze() - w_min) / (w_max - w_min)\n",
        "    template = template.astype(np.uint8)\n",
        "    plt.imshow(template)\n",
        "    plt.title(cifar10.LABELS[idx])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# TODO your code here\n",
        "# use the metrics module to compute the precision, recall and confusion matrix for the best classifier\n",
        "# end TODO your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwVLNAq7JPvU"
      },
      "source": [
        "## Classifier evaluation\n",
        " \n",
        "After the training process, you want to evaluate the model on the test set, such that you can get an idea on how well your model will perform on unseen data.\n",
        "Ideally the classes in the test set should be balanced (i.e. you should have the same number of samples for each one of the classes).\n",
        " \n",
        "### Confusion matrix and classification metrics\n",
        " \n",
        "The confusion matrix can be considered the foundation stone for evaluating a classifier. As the name states, it's a simple way of visualising whether/how the model is confusing the classes. \n",
        " \n",
        "Each row of the confusion matrix represents the instances of the ground truth class, while each column represents the instances of the predicted class.\n",
        " \n",
        "<img src=\"https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg\"/>\n",
        " \n",
        "Based on the confusion matrix, you can compute different classification metrics:\n",
        "* *accuracy*: this is simply the ratio between the correctly classified samples (either positive or negative) and the total number of samples;\n",
        "* _precisi**on**_ : this metric measures the ability of the classifier to capture **only** relevant samples;\n",
        "* _rec**all**_ : this metric measures the ability of the classifier to spot **all** positive samples.\n",
        " \n",
        "As you may have noticed, it is not possible to maximize precision and recall at the same time, as one comes at the cost of another. \n",
        "Therefore, the $F_1$ score -- the harmonic mean between precision and recall -- was defined to combine these two metrics into a single numerical value.\n",
        " \n",
        "\\begin{equation}\n",
        "F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\n",
        "\\end{equation}\n",
        " \n",
        "In the file *metrics.py* you should fill in the code for computing the accuracy, precision, recall and f1-score of your classifier.\n",
        " \n",
        "One challenge when implementing these metrics is that you are not allowed to use any repetitive loops (only numpy vectorization).\n",
        " \n",
        "\n",
        "## Additional questions\n",
        " \n",
        "You want to measure the air quality in your city, so you bought an air quality sensor. \n",
        " \n",
        "However, you notice that your sensor always returns an air quality index equal to 75, no matter where you place it.\n",
        " \n",
        "* What can you say about the precision of the measurements that you perform?\n",
        "What about the accuracy of these measurements?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2CaJ3AhqHV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0y6fkhMEcUS"
      },
      "source": [
        "Compute the metrics on the best classifier you obtained so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o988iS9KM_RY"
      },
      "source": [
        "Your answer = 'my answer' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6_XFTsTNAXM"
      },
      "source": [
        "You determined that the sensor is broken, so you change it with a brand new one. Now everything seems to be ok.\n",
        " \n",
        "To measure the air quality around your house, you place the sensor in different areas: near your favourite scented candle, under your gas central heating exhaust pipe, on your balcony oriented towards the forest/ocean/mountains :) etc.\n",
        " \n",
        "* What can you say about the precision of the measurements that you perform? What about the accuracy of these measurements?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "OOSlFpD8NwBm"
      },
      "source": [
        "Your answer = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR8DVcLpUiCl"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxc90HHTVELE"
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 100%; color: red;'><b>Yay! You're done!</b></marquee>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJdGlrgPVX7O"
      },
      "source": [
        "Please take two minutes of your time and fill in this [form](https://docs.google.com/forms/d/1sjQZKtJ0dbzeZbdihyAd2O04btAx3hbs7A37eXVDcuI) related to this lab. \n",
        "\n",
        "Thanks for your valuable feedback and for helping me \"test\" the labs!\n",
        "\n",
        "<img src=\"https://i.imgflip.com/2uxn79.jpg\" style=\"width:300px;\"/>\n"
      ]
    }
  ]
}